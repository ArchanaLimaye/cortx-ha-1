#!/bin/bash

set -eu -o pipefail

HA_PATH="<HA_PATH>"

init() {
    echo "Setup Decision maker"
    host1=$(grep left-node $ARGS | awk '{print $2}')
    host2=$(grep right-node $ARGS | awk '{print $2}')
    uuid1=$(salt "*" grains.get node_id --output=json | jq '.["'$host1'"]')
    uuid2=$(salt "*" grains.get node_id --output=json | jq '.["'$host2'"]')

    echo "Reading cluster.sls"
    node1_data_nw=$(salt-call --local pillar.get cluster:$host1 --output=json | jq '.["local"].network["data_nw"].iface')
    node1_mgmt_nw=$(salt-call --local pillar.get cluster:$host1 --output=json | jq '.["local"].network["mgmt_nw"].iface')
    node2_data_nw=$(salt-call --local pillar.get cluster:$host2 --output=json | jq '.["local"].network["data_nw"].iface')
    node2_mgmt_nw=$(salt-call --local pillar.get cluster:$host2 --output=json | jq '.["local"].network["mgmt_nw"].iface')

    decision_monitor_conf=${HA_PATH}/conf/decision_monitor_conf.json
    cp -rf $decision_monitor_conf /tmp/decision_monitor_conf1.json
    cp -rf $decision_monitor_conf /tmp/decision_monitor_conf2.json

    sed -i -e "s|<LOCAL>|${host1}|g" \
        -e "s|<HOST1>|${host1}|g" \
        -e "s|<HOST2>|${host2}|g" /tmp/decision_monitor_conf1.json
    sed -i -e "s|\"<UUID1>\"|${uuid1//$'\n'}|g" -e "s|null||g" /tmp/decision_monitor_conf1.json
    sed -i -e "s|\"<UUID2>\"|${uuid2//$'\n'}|g" -e "s|null||g" /tmp/decision_monitor_conf1.json

    sed -i -e "s|<LOCAL>|${host2}|g" \
        -e "s|<HOST1>|${host1}|g" \
        -e "s|<HOST2>|${host2}|g" /tmp/decision_monitor_conf2.json
    sed -i -e "s|\"<UUID1>\"|${uuid1//$'\n'}|g" -e "s|null||g" /tmp/decision_monitor_conf2.json
    sed -i -e "s|\"<UUID2>\"|${uuid2//$'\n'}|g" -e "s|null||g" /tmp/decision_monitor_conf2.json

    sed -i -e "s|\"<N1_DATA_IFACE>\"|${node1_data_nw//$'\n'/}|g" \
        -e "s|\"<N1_MGMT_IFACE>\"|${node1_mgmt_nw//$'\n'/}|g" \
        -e "s|\"<N2_DATA_IFACE>\"|${node2_data_nw//$'\n'/}|g" \
        -e "s|\"<N2_MGMT_IFACE>\"|${node2_mgmt_nw//$'\n'/}|g" /tmp/decision_monitor_conf1.json

    sed -i -e "s|\"<N1_DATA_IFACE>\"|${node1_data_nw//$'\n'/}|g" \
        -e "s|\"<N1_MGMT_IFACE>\"|${node1_mgmt_nw//$'\n'/}|g" \
        -e "s|\"<N2_DATA_IFACE>\"|${node2_data_nw//$'\n'/}|g" \
        -e "s|\"<N2_MGMT_IFACE>\"|${node2_mgmt_nw//$'\n'/}|g" /tmp/decision_monitor_conf2.json

    cp -rf /tmp/decision_monitor_conf1.json /etc/cortx/ha/decision_monitor_conf.json
    scp -r /tmp/decision_monitor_conf2.json $host2:/etc/cortx/ha/decision_monitor_conf.json
    rm -rf /tmp/decision_monitor_conf1.json /tmp/decision_monitor_conf2.json

    pcs cluster cib hw_cfg
    pcs -f hw_cfg resource create io_path_health-c1 ocf:seagate:hw_comp_ra path='io' filename='io_path_health-c1' op monitor timeout=30s interval=30s
    pcs -f hw_cfg resource create io_path_health-c2 ocf:seagate:hw_comp_ra path='io' filename='io_path_health-c2' op monitor timeout=30s interval=30s
    pcs -f hw_cfg resource create mgmt_path_health-c1 ocf:seagate:hw_comp_ra path='mgmt' filename='mgmt_path_health-c1' op monitor timeout=30s interval=30s

    pcs -f hw_cfg constraint colocation add io_path_health-c1 with consul-c1
    pcs -f hw_cfg constraint colocation add io_path_health-c2 with consul-c2
    pcs -f hw_cfg resource group add csm-kibana mgmt_path_health-c1

    pcs -f hw_cfg constraint order consul-c1 then io_path_health-c1
    pcs -f hw_cfg constraint order consul-c2 then io_path_health-c2
    pcs -f hw_cfg constraint order consul-c1 then mgmt_path_health-c1
    pcs cluster cib-push hw_cfg

    pcs cluster cib iem_cfg
    pcs -f iem_cfg resource create node_iem_mero-c1 ocf:seagate:iem_comp_ra path='node_iem_mero' filename='node_iem_mero-c1' node=$host1 op monitor timeout=30s interval=30s
    pcs -f iem_cfg resource create node_iem_mero-c2 ocf:seagate:iem_comp_ra path='node_iem_mero' filename='node_iem_mero-c2' node=$host2 op monitor timeout=30s interval=30s

    pcs -f iem_cfg constraint colocation add node_iem_mero-c1 with consul-c1
    pcs -f iem_cfg constraint colocation add node_iem_mero-c2 with consul-c2

    pcs -f iem_cfg constraint order consul-c1 then node_iem_mero-c1
    pcs -f iem_cfg constraint order consul-c2 then node_iem_mero-c2

    pcs -f iem_cfg resource create node_iem_s3 ocf:seagate:iem_comp_ra path='node_iem_s3' filename='node_iem_s3' service="slapd" op monitor timeout=30s interval=30s
    pcs -f iem_cfg resource clone node_iem_s3 clone-max=2 clone-node-max=1
    pcs -f iem_cfg constraint location node_iem_s3-clone prefers ${host1}=INFINITY
    pcs -f iem_cfg constraint location node_iem_s3-clone prefers ${host2}=INFINITY
    pcs cluster cib-push iem_cfg
}

cleanup() {
    echo "Delete resources"
    resources=(
        io_path_health-c1
        io_path_health-c2
        mgmt_path_health-c1
        node_iem_mero-c1
        node_iem_mero-c2
        node_iem_s3
    )
    for r in ${resources[@]}; do
        pcs resource delete $r &
    done
    wait
}

# TODO: remove ARGS(hare file) dependency from cortx-ha
ACTION=$1
ARGS=$2

$ACTION $ARGS
